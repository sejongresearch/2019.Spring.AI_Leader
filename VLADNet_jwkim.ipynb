{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VLADNet_jwkim.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suimn416/2019.Spring.AI_Leader/blob/master/VLADNet_jwkim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVFa5HXlsEK1",
        "colab_type": "code",
        "outputId": "99fa09a2-d321-4936-a7a6-754356ba8df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqsULtEewAJy",
        "colab_type": "code",
        "outputId": "68caa15f-f917-4603-e354-38064ab0b66e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/gdrive')==False:\n",
        "    drive.mount('/content/gdrive')\n",
        "    print('Google Drive is mounted\\n')\n",
        "else:\n",
        "    print('Google Drive is already mounted\\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Google Drive is mounted\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrfD-ILt-2BN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "List_C_door = np.load('./gdrive/My Drive/AILeader_Dataset/어린이대공원정문.npy')\n",
        "List_door = np.load('./gdrive/My Drive/AILeader_Dataset/세종대정문.npy')\n",
        "AI_center = np.load('./gdrive/My Drive/AILeader_Dataset/대양AI센터.npy')\n",
        "Museum = np.load('./gdrive/My Drive/AILeader_Dataset/박물관.npy')\n",
        "C_Tower = np.load('./gdrive/My Drive/AILeader_Dataset/세종대 시계탑.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ByNd9eMsFYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetVLAD(nn.Module):\n",
        "    \"\"\"NetVLAD layer implementation\"\"\"\n",
        "\n",
        "    def __init__(self, num_clusters=64, dim=128, alpha=100.0,\n",
        "                 normalize_input=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_clusters : int\n",
        "                The number of clusters\n",
        "            dim : int\n",
        "                Dimension of descriptors\n",
        "            alpha : float\n",
        "                Parameter of initialization. Larger value is harder assignment.\n",
        "            normalize_input : bool\n",
        "                If true, descriptor-wise L2 normalization is applied to input.\n",
        "        \"\"\"\n",
        "        super(NetVLAD, self).__init__()\n",
        "        self.num_clusters = num_clusters\n",
        "        self.dim = dim\n",
        "        self.alpha = alpha\n",
        "        self.normalize_input = normalize_input\n",
        "        self.conv = nn.Conv2d(dim, num_clusters, kernel_size=(1, 1), bias=True)\n",
        "        self.centroids = nn.Parameter(torch.rand(num_clusters, dim))\n",
        "        self._init_params()\n",
        "\n",
        "    def _init_params(self):\n",
        "        self.conv.weight = nn.Parameter(\n",
        "            (2.0 * self.alpha * self.centroids).unsqueeze(-1).unsqueeze(-1)\n",
        "        )\n",
        "        self.conv.bias = nn.Parameter(\n",
        "            - self.alpha * self.centroids.norm(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C = x.shape[:2]\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = F.normalize(x, p=2, dim=1)  # across descriptor dim\n",
        "\n",
        "        # soft-assignment\n",
        "        soft_assign = self.conv(x).view(N, self.num_clusters, -1)\n",
        "        soft_assign = F.softmax(soft_assign, dim=1)\n",
        "\n",
        "        x_flatten = x.view(N, C, -1)\n",
        "        \n",
        "        # calculate residuals to each clusters\n",
        "        residual = x_flatten.expand(self.num_clusters, -1, -1, -1).permute(1, 0, 2, 3) - \\\n",
        "            self.centroids.expand(x_flatten.size(-1), -1, -1).permute(1, 2, 0).unsqueeze(0)\n",
        "        residual *= soft_assign.unsqueeze(2)\n",
        "        vlad = residual.sum(dim=-1)\n",
        "\n",
        "        vlad = F.normalize(vlad, p=2, dim=2)  # intra-normalization\n",
        "        vlad = vlad.view(x.size(0), -1)  # flatten\n",
        "        vlad = F.normalize(vlad, p=2, dim=1)  # L2 normalize\n",
        "\n",
        "        return vlad\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKzKKFWksfVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbedNet(nn.Module):\n",
        "    def __init__(self, base_model, net_vlad):\n",
        "        super(EmbedNet, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.net_vlad = net_vlad\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        embedded_x = self.net_vlad(x)\n",
        "        return embedded_x\n",
        "      \n",
        "class TripletNet(nn.Module):\n",
        "    def __init__(self, embed_net):\n",
        "        super(TripletNet, self).__init__()\n",
        "        self.embed_net = embed_net\n",
        "\n",
        "    def forward(self, a, p, n):\n",
        "        embedded_a = self.embed_net(a)\n",
        "        embedded_p = self.embed_net(p)\n",
        "        embedded_n = self.embed_net(n)\n",
        "        return embedded_a, embedded_p, embedded_n\n",
        "\n",
        "    def feature_extract(self, x):\n",
        "        return self.embed_net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD_yKdlMsZai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HardTripletLoss(nn.Module):\n",
        "    \"\"\"Hard/Hardest Triplet Loss\n",
        "    (pytorch implementation of https://omoindrot.github.io/triplet-loss)\n",
        "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
        "    \"\"\"\n",
        "    def __init__(self, margin=0.1, hardest=False, squared=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            margin: margin for triplet loss\n",
        "            hardest: If true, loss is considered only hardest triplets.\n",
        "            squared: If true, output is the pairwise squared euclidean distance matrix.\n",
        "                If false, output is the pairwise euclidean distance matrix.\n",
        "        \"\"\"\n",
        "        super(HardTripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.hardest = hardest\n",
        "        self.squared = squared\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            labels: labels of the batch, of size (batch_size,)\n",
        "            embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        Returns:\n",
        "            triplet_loss: scalar tensor containing the triplet loss\n",
        "        \"\"\"\n",
        "        pairwise_dist = _pairwise_distance(embeddings, squared=self.squared)\n",
        "\n",
        "        if self.hardest:\n",
        "            # Get the hardest positive pairs\n",
        "            mask_anchor_positive = _get_anchor_positive_triplet_mask(labels).float()\n",
        "            valid_positive_dist = pairwise_dist * mask_anchor_positive\n",
        "            hardest_positive_dist, _ = torch.max(valid_positive_dist, dim=1, keepdim=True)\n",
        "\n",
        "            # Get the hardest negative pairs\n",
        "            mask_anchor_negative = _get_anchor_negative_triplet_mask(labels).float()\n",
        "            max_anchor_negative_dist, _ = torch.max(pairwise_dist, dim=1, keepdim=True)\n",
        "            anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (\n",
        "                    1.0 - mask_anchor_negative)\n",
        "            hardest_negative_dist, _ = torch.min(anchor_negative_dist, dim=1, keepdim=True)\n",
        "\n",
        "            # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
        "            triplet_loss = F.relu(hardest_positive_dist - hardest_negative_dist + 0.1)\n",
        "            triplet_loss = torch.mean(triplet_loss)\n",
        "        else:\n",
        "            anc_pos_dist = pairwise_dist.unsqueeze(dim=2)\n",
        "            anc_neg_dist = pairwise_dist.unsqueeze(dim=1)\n",
        "\n",
        "            # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
        "            # triplet_loss[i, j, k] will contain the triplet loss of anc=i, pos=j, neg=k\n",
        "            # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
        "            # and the 2nd (batch_size, 1, batch_size)\n",
        "            loss = anc_pos_dist - anc_neg_dist + self.margin\n",
        "\n",
        "            mask = _get_triplet_mask(labels).float()\n",
        "            triplet_loss = loss * mask\n",
        "\n",
        "            # Remove negative losses (i.e. the easy triplets)\n",
        "            triplet_loss = F.relu(triplet_loss)\n",
        "\n",
        "            # Count number of hard triplets (where triplet_loss > 0)\n",
        "            hard_triplets = torch.gt(triplet_loss, 1e-16).float()\n",
        "            num_hard_triplets = torch.sum(hard_triplets)\n",
        "\n",
        "            triplet_loss = torch.sum(triplet_loss) / (num_hard_triplets + 1e-16)\n",
        "\n",
        "        return triplet_loss\n",
        "\n",
        "\n",
        "def _pairwise_distance(x, squared=False, eps=1e-16):\n",
        "    # Compute the 2D matrix of distances between all the embeddings.\n",
        "\n",
        "    cor_mat = torch.matmul(x, x.t())\n",
        "    norm_mat = cor_mat.diag()\n",
        "    distances = norm_mat.unsqueeze(1) - 2 * cor_mat + norm_mat.unsqueeze(0)\n",
        "    distances = F.relu(distances)\n",
        "\n",
        "    if not squared:\n",
        "        mask = torch.eq(distances, 0.0).float()\n",
        "        distances = distances + mask * eps\n",
        "        distances = torch.sqrt(distances)\n",
        "        distances = distances * (1.0 - mask)\n",
        "\n",
        "    return distances\n",
        "\n",
        "\n",
        "def _get_anchor_positive_triplet_mask(labels):\n",
        "    # Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    indices_not_equal = torch.eye(labels.shape[0]).to(device).byte() ^ 1\n",
        "\n",
        "    # Check if labels[i] == labels[j]\n",
        "    labels_equal = torch.unsqueeze(labels, 0) == torch.unsqueeze(labels, 1)\n",
        "\n",
        "    mask = indices_not_equal * labels_equal\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _get_anchor_negative_triplet_mask(labels):\n",
        "    # Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
        "\n",
        "    # Check if labels[i] != labels[k]\n",
        "    labels_equal = torch.unsqueeze(labels, 0) == torch.unsqueeze(labels, 1)\n",
        "    mask = labels_equal ^ 1\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _get_triplet_mask(labels):\n",
        "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
        "    A triplet (i, j, k) is valid if:\n",
        "        - i, j, k are distinct\n",
        "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Check that i, j and k are distinct\n",
        "    indices_not_same = torch.eye(labels.shape[0]).to(device).byte() ^ 1\n",
        "    i_not_equal_j = torch.unsqueeze(indices_not_same, 2)\n",
        "    i_not_equal_k = torch.unsqueeze(indices_not_same, 1)\n",
        "    j_not_equal_k = torch.unsqueeze(indices_not_same, 0)\n",
        "    distinct_indices = i_not_equal_j * i_not_equal_k * j_not_equal_k\n",
        "\n",
        "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    label_equal = torch.eq(torch.unsqueeze(labels, 0), torch.unsqueeze(labels, 1))\n",
        "    i_equal_j = torch.unsqueeze(label_equal, 2)\n",
        "    i_equal_k = torch.unsqueeze(label_equal, 1)\n",
        "    valid_labels = i_equal_j * (i_equal_k ^ 1)\n",
        "\n",
        "    mask = distinct_indices * valid_labels   # Combine the two masks\n",
        "\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPT97DFbsK7w",
        "colab_type": "code",
        "outputId": "bcdfddb8-1b38-4e74-fbd7-6820f923a293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Discard layers at the end of base network\n",
        "encoder = resnet18(pretrained=True)\n",
        "base_model = nn.Sequential(\n",
        "    encoder.conv1,\n",
        "    encoder.bn1,\n",
        "    encoder.relu,\n",
        "    encoder.maxpool,\n",
        "    encoder.layer1,\n",
        "    encoder.layer2,\n",
        "    encoder.layer3,\n",
        "    encoder.layer4,\n",
        ")\n",
        "dim = list(base_model.parameters())[-1].shape[0]  # last channels (512)\n",
        "\n",
        "# Define model for embedding\n",
        "net_vlad = NetVLAD(num_clusters=5, dim=dim, alpha=1.0)\n",
        "model = EmbedNet(base_model, net_vlad).cuda()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 46827520/46827520 [00:00<00:00, 168606487.66it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxHsPHtrsp4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss\n",
        "criterion = HardTripletLoss(margin=0.1).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUphLuCaLMuT",
        "colab_type": "code",
        "outputId": "25e49834-8417-4d78-8301-3ec7a4091bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(List_door.shape,List_C_door.shape,AI_center.shape,Museum.shape,C_Tower.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(98, 3, 128, 128) (170, 3, 128, 128) (184, 3, 128, 128) (110, 3, 128, 128) (109, 3, 128, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1_C2i2l7cV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_image_set = np.concatenate((List_door[:80], List_C_door[:80],AI_center[:80],Museum[:80],C_Tower[:80]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejgb_fHl7z-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_image_set = torch.from_numpy(train_image_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx6ujJbm8vxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_image_set = train_image_set.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ji9rrNN8-8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_train = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "                  1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
        "                  2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,\n",
        "                  3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,\n",
        "                  4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwaU5pM59N_9",
        "colab_type": "code",
        "outputId": "0d6bb6f1-44ca-4650-fc20-68b1d168dbe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "label_train = torch.from_numpy(label_train)\n",
        "label_train = label_train.cuda()\n",
        "label_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJkRFjLlMEcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 70\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQPFdi7QL-uH",
        "colab_type": "code",
        "outputId": "609c52fc-eca0-4d85-da05-fa8f25a136f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1277
        }
      },
      "source": [
        "for ii in range(epoch):\n",
        "  output_train = model(train_image_set)\n",
        "  triplet_loss = criterion(output_train, label_train)\n",
        "  optimizer.zero_grad()\n",
        "  triplet_loss.backward(retain_graph=True)\n",
        "  optimizer.step()\n",
        "  \n",
        "  print(ii,triplet_loss)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.0984, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "1 tensor(0.0982, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "2 tensor(0.0981, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "3 tensor(0.0980, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "4 tensor(0.0978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "5 tensor(0.0977, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "6 tensor(0.0975, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "7 tensor(0.0974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "8 tensor(0.0972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "9 tensor(0.0970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "10 tensor(0.0968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "11 tensor(0.0966, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "12 tensor(0.0963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "13 tensor(0.0961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "14 tensor(0.0958, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "15 tensor(0.0956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "16 tensor(0.0953, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "17 tensor(0.0950, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "18 tensor(0.0946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "19 tensor(0.0943, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "20 tensor(0.0939, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "21 tensor(0.0935, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "22 tensor(0.0931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "23 tensor(0.0926, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "24 tensor(0.0921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "25 tensor(0.0916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "26 tensor(0.0911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "27 tensor(0.0905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "28 tensor(0.0899, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "29 tensor(0.0893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "30 tensor(0.0885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "31 tensor(0.0878, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "32 tensor(0.0870, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "33 tensor(0.0862, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "34 tensor(0.0852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "35 tensor(0.0843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "36 tensor(0.0831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "37 tensor(0.0820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "38 tensor(0.0807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "39 tensor(0.0795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "40 tensor(0.0781, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "41 tensor(0.0769, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "42 tensor(0.0747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "43 tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "44 tensor(0.0702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "45 tensor(0.0676, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "46 tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "47 tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "48 tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "49 tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "50 tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "51 tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "52 tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "53 tensor(0.0370, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "54 tensor(0.0336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "55 tensor(0.0334, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "56 tensor(0.0283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "57 tensor(0.0250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "58 tensor(0.0228, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "59 tensor(0.0208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "60 tensor(0.0189, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "61 tensor(0.0189, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "62 tensor(0.0166, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "63 tensor(0.0132, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "64 tensor(0.0118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "65 tensor(0.0105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "66 tensor(0.0101, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "67 tensor(0.0093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "68 tensor(0.0094, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "69 tensor(0.0079, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0kw_RpmQbV_",
        "colab_type": "code",
        "outputId": "a736cd3c-09c0-4606-ddd8-3bfe0fe4d8c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_image_set = np.concatenate((List_door[-15:], List_C_door[-15:],AI_center[-15:],Museum[-15:],C_Tower[-15:]))\n",
        "test_image_set = torch.from_numpy(test_image_set)\n",
        "test_image_set = test_image_set.cuda()\n",
        "print(List_door[-15:].shape,List_C_door[-15:].shape,AI_center[-15:].shape,Museum[-15:].shape,C_Tower[-15:].shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15, 3, 128, 128) (15, 3, 128, 128) (15, 3, 128, 128) (15, 3, 128, 128) (15, 3, 128, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HdKVqIVQmbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_test = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "                  1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
        "                  2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,\n",
        "                  3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,\n",
        "                  4,4,4,4,4,4,4,4,4,4,4,4,4,4,4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxt_tbGkT3KA",
        "colab_type": "code",
        "outputId": "c0cd6156-7e6c-4e18-e2fb-4a822f8bade2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "label_test = torch.from_numpy(label_test)\n",
        "label_test = label_test.cuda()\n",
        "label_test.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([75])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-SC7xaGMYJm",
        "colab_type": "code",
        "outputId": "2b27a276-ebde-4163-913d-ec8747e88f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "output_test = model(test_image_set)\n",
        "output_test.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([75, 2560])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD7yhbTHOmj9",
        "colab_type": "code",
        "outputId": "7a2d0814-a2d4-4d17-83db-828d11e4e2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets, metrics\n",
        "\n",
        "X = output_train.cpu().data.numpy()\n",
        "Y = label_train.cpu().data.numpy()\n",
        "\n",
        "clf = svm.SVC(gamma='scale', decision_function_shape='ovo')\n",
        "clf.fit(X, Y) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRL85iOzUQu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = output_test.cpu().data.numpy()\n",
        "Y_test = label_test.cpu().data.numpy()\n",
        "\n",
        "pred = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxogKchJZDZM",
        "colab_type": "code",
        "outputId": "93fb34dc-88c2-4ac2-f49d-86200aca1121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(metrics.classification_report(Y_test,pred))\n",
        "print(metrics.confusion_matrix(Y_test,pred))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.80      0.83        15\n",
            "           1       0.73      0.73      0.73        15\n",
            "           2       0.75      0.80      0.77        15\n",
            "           3       0.56      0.67      0.61        15\n",
            "           4       0.58      0.47      0.52        15\n",
            "\n",
            "    accuracy                           0.69        75\n",
            "   macro avg       0.70      0.69      0.69        75\n",
            "weighted avg       0.70      0.69      0.69        75\n",
            "\n",
            "[[12  1  0  2  0]\n",
            " [ 0 11  0  3  1]\n",
            " [ 0  1 12  0  2]\n",
            " [ 0  2  1 10  2]\n",
            " [ 2  0  3  3  7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqKcZT_-ZHTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}